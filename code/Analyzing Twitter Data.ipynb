{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Trolls\n",
    "### Classifying and analyzing Russian Troll Tweets using Deep Learning\n",
    "#### by Christopher DeCarolis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "We have two different data sets to work with. The first is a set of Tweets scraped from Russian Troll accounts (link __[here](https://www.kaggle.com/vikasg/russian-troll-tweets)__), and the second is a set of tweets scraped from random accounts during election day (link __[here](https://www.kaggle.com/kinguistics/election-day-tweets)__). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_russian = pd.read_csv('~/.kaggle/datasets/vikasg/russian-troll-tweets/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id         user_key    created_at          created_str  \\\n",
      "1  2.571870e+09  detroitdailynew  1.476133e+12  2016-10-10 20:57:00   \n",
      "2  1.710805e+09       cookncooks  1.487767e+12  2017-02-22 12:43:43   \n",
      "3  2.584153e+09     queenofthewo  1.482765e+12  2016-12-26 15:06:41   \n",
      "4  1.768260e+09     mrclydepratt  1.501987e+12  2017-08-06 02:36:24   \n",
      "5  2.882014e+09      giselleevns  1.477496e+12  2016-10-26 15:33:58   \n",
      "6  1.658421e+09        baobaeham  1.488910e+12  2017-03-07 18:11:44   \n",
      "7  2.587101e+09   judelambertusa  1.483102e+12  2016-12-30 12:49:30   \n",
      "8  1.679279e+09    ameliebaldwin  1.477792e+12  2016-10-30 01:48:19   \n",
      "9  1.649488e+09        hiimkhloe  1.458155e+12  2016-03-16 19:07:39   \n",
      "\n",
      "   retweet_count retweeted  favorite_count  \\\n",
      "1            0.0     False             0.0   \n",
      "2            NaN       NaN             NaN   \n",
      "3            NaN       NaN             NaN   \n",
      "4            NaN       NaN             NaN   \n",
      "5            NaN       NaN             NaN   \n",
      "6            NaN       NaN             NaN   \n",
      "7            NaN       NaN             NaN   \n",
      "8            0.0     False             0.0   \n",
      "9            NaN       NaN             NaN   \n",
      "\n",
      "                                                text      tweet_id  \\\n",
      "1  Clinton: Trump shouldâ€™ve apologized more, atta...  7.855849e+17   \n",
      "2  RT @ltapoll: Who was/is the best president of ...  8.343832e+17   \n",
      "3  RT @jww372: I don't have to guess your religio...  8.134006e+17   \n",
      "4  RT @Shareblue: Pence and his lawyers decided w...  8.940243e+17   \n",
      "5                             @ModicaGiunta me, too!  7.913019e+17   \n",
      "6  RT @MDBlanchfield: Youâ€™ll never guess who twee...  8.391768e+17   \n",
      "7  RT @100PercFEDUP: New post: WATCH: DIAMOND AND...  8.148157e+17   \n",
      "8  RT @AriaWilsonGOP: 3 Women Face Charges After ...  7.925436e+17   \n",
      "9  One of the ways to remind that #BlackLivesMatt...  7.101807e+17   \n",
      "\n",
      "                                              source                hashtags  \\\n",
      "1  <a href=\"http://twitterfeed.com\" rel=\"nofollow...                      []   \n",
      "2                                                NaN                      []   \n",
      "3                                                NaN  [\"ChristmasAftermath\"]   \n",
      "4                                                NaN                      []   \n",
      "5                                                NaN                      []   \n",
      "6                                                NaN                      []   \n",
      "7                                                NaN                      []   \n",
      "8  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...                      []   \n",
      "9                                                NaN    [\"Blacklivesmatter\"]   \n",
      "\n",
      "                          expanded_urls  posted           mentions  \\\n",
      "1           [\"http://detne.ws/2e172jF\"]  POSTED                 []   \n",
      "2                                    []  POSTED                 []   \n",
      "3                                    []  POSTED                 []   \n",
      "4                                    []  POSTED                 []   \n",
      "5                                    []  POSTED                 []   \n",
      "6                                    []  POSTED                 []   \n",
      "7                                    []  POSTED                 []   \n",
      "8  [\"http://www.Feed24hNews.com/4MzaL\"]  POSTED  [\"ariawilsongop\"]   \n",
      "9                                    []  POSTED                 []   \n",
      "\n",
      "   retweeted_status_id  in_reply_to_status_id  \n",
      "1                  NaN                    NaN  \n",
      "2                  NaN                    NaN  \n",
      "3                  NaN                    NaN  \n",
      "4                  NaN                    NaN  \n",
      "5                  NaN                    NaN  \n",
      "6                  NaN                    NaN  \n",
      "7                  NaN                    NaN  \n",
      "8         7.925391e+17                    NaN  \n",
      "9                  NaN                    NaN  \n"
     ]
    }
   ],
   "source": [
    "print(df_russian[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_election = pd.read_csv('~/.kaggle/datasets/kinguistics/election-day-tweets/election_day_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text           created_at  \\\n",
      "1  My @latimesopinion op-ed on historic #Californ...  2016-11-08 04:08:10   \n",
      "2  #Senate Wisconsin Senate Preview: Johnson vs. ...  2016-11-08 04:11:35   \n",
      "3  If Rubio Wins and #Trump Loses in #Florida... ...  2016-11-08 04:12:16   \n",
      "4  #Senate Wisconsin Senate Preview: Johnson vs. ...  2016-11-08 04:16:20   \n",
      "5  bob day is an \"honest  person \"  #senate patte...  2016-11-08 04:18:55   \n",
      "6  Make Republicans #PayAPrice!\\n ðŸ’™ðŸ‡ºðŸ‡¸#VoteBLUEðŸ”ƒth...  2016-11-08 04:20:09   \n",
      "7  She's done America!! Please vote for @realDona...  2016-11-08 04:20:43   \n",
      "8  #Illinois #Senate #StrongerTogether https://t....  2016-11-08 04:26:36   \n",
      "9  #Senate Sen. Mark Warner to speak at ODU for V...  2016-11-08 04:41:04   \n",
      "\n",
      "   geo lang place coordinates  user.favourites_count  user.statuses_count  \\\n",
      "1  NaN   en   NaN         NaN                      8                 4841   \n",
      "2  NaN   en   NaN         NaN                    728               160390   \n",
      "3  NaN   en   NaN         NaN                      0                18105   \n",
      "4  NaN   en   NaN         NaN                   4722               247322   \n",
      "5  NaN   en   NaN         NaN                  15171               366147   \n",
      "6  NaN   en   NaN         NaN                  29850                30278   \n",
      "7  NaN   en   NaN         NaN                    389                  299   \n",
      "8  NaN  und   NaN         NaN                    464                 4282   \n",
      "9  NaN   en   NaN         NaN                    728               160390   \n",
      "\n",
      "                                    user.description  \\\n",
      "1  Hoover Institution research fellow; https://t....   \n",
      "2  US SENATE NEWS FEED.  The U.S. Senate is the w...   \n",
      "3  The most trending Marco Rubio news as collecte...   \n",
      "4  News & #ALERTS. We are a large group of proud ...   \n",
      "5  jazz trumpet ,fluglehorn , biker ,barrister, A...   \n",
      "6                                        Born to Fly   \n",
      "7                                                NaN   \n",
      "8  An advocate for high quality of life and equal...   \n",
      "9  US SENATE NEWS FEED.  The U.S. Senate is the w...   \n",
      "\n",
      "                   user.location      ...        user.geo_enabled  \\\n",
      "1          Palo Alto, California      ...                    True   \n",
      "2                            USA      ...                   False   \n",
      "3                  Washington DC      ...                   False   \n",
      "4  24hr Live HD Stream Broadcast      ...                   False   \n",
      "5         Mirboo North, Victoria      ...                    True   \n",
      "6                             FL      ...                   False   \n",
      "7                   Georgia, USA      ...                   False   \n",
      "8                    Chicago, IL      ...                    True   \n",
      "9                            USA      ...                   False   \n",
      "\n",
      "  user.profile_background_color  \\\n",
      "1                        C0DEED   \n",
      "2                        473969   \n",
      "3                        C0DEED   \n",
      "4                        1B95E0   \n",
      "5                        9AE4E8   \n",
      "6                        F5F8FA   \n",
      "7                        352726   \n",
      "8                        E2B9ED   \n",
      "9                        473969   \n",
      "\n",
      "                              user.profile_image_url  \\\n",
      "1  http://pbs.twimg.com/profile_images/6514470051...   \n",
      "2  http://pbs.twimg.com/profile_images/6155877017...   \n",
      "3  http://pbs.twimg.com/profile_images/5890065307...   \n",
      "4  http://pbs.twimg.com/profile_images/7971276617...   \n",
      "5  http://pbs.twimg.com/profile_images/6690303705...   \n",
      "6  http://pbs.twimg.com/profile_images/7236248390...   \n",
      "7  http://pbs.twimg.com/profile_images/7928972552...   \n",
      "8  http://pbs.twimg.com/profile_images/1060357758...   \n",
      "9  http://pbs.twimg.com/profile_images/6155877017...   \n",
      "\n",
      "               user.time_zone                  id  favorite_count  retweeted  \\\n",
      "1  Pacific Time (US & Canada)  795840310597193728               3      False   \n",
      "2                      Alaska  795841168621719552               0      False   \n",
      "3                         NaN  795841341146234880               0      False   \n",
      "4  Eastern Time (US & Canada)  795842365881782272               0      False   \n",
      "5                   Melbourne  795843014954348544               1      False   \n",
      "6                         NaN  795843326192783360               1      False   \n",
      "7  Eastern Time (US & Canada)  795843467352084480               0      False   \n",
      "8  Central Time (US & Canada)  795844950072102916               0      False   \n",
      "9                      Alaska  795848590685978628               0      False   \n",
      "\n",
      "                        source  favorited  retweet_count  \n",
      "1           Twitter Web Client      False              4  \n",
      "2                      dlvr.it      False              0  \n",
      "3  Lead Stories Feed Publisher      False              0  \n",
      "4                        IFTTT      False              0  \n",
      "5           Twitter Web Client      False              0  \n",
      "6              Mobile Web (M5)      False              0  \n",
      "7           Twitter for iPhone      False              0  \n",
      "8           Twitter for iPhone      False              0  \n",
      "9                      dlvr.it      False              0  \n",
      "\n",
      "[9 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_election[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the first few rows of each dataset, we can see that they were properly imported in terms of format. Something important to note about the election dataset is that we technically have know way of knowing whether the data collected comes from legitimate accounts or note. It is possible (likely even) that some of the users in this dataset were actually troll accounts. However, as we have know way of definitively telling that, we will proceed under the assumption that the election day tweets dataset represents tweets from real individuals.\n",
    "Now we need to split the datasets into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_russian = shuffle(df_russian)\n",
    "df_election = shuffle(df_election)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Obama on Trump winning: 'Anything's possible' https://t.co/MjVMZ5TR8Y #politics\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_russian.loc[20, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_russian_df, test_russian_df = train_test_split(df_russian.loc[:, 'text'], test_size=0.2)\n",
    "train_election_df, test_election_df = train_test_split(df_election.loc[:, 'text'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Text Cleaning__: Below we remove all hashtags and unnecessary filler from the text that would otherwise hinder classification. We also perform a train-test split on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of English language contractions\n",
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"obama trump winning anything's possible https://t.co/mjvmz5tr8y #politics\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # make sure that text is in string format\n",
    "    text = str(text)\n",
    "    new_text = []\n",
    "    tokenized_text = casual_tokenize(text)\n",
    "    for token in tokenized_text:\n",
    "        # get rid of contractions\n",
    "        if token in contractions:\n",
    "            new_text.extend(token.split())\n",
    "        # if string is not empty then add it\n",
    "        elif not token == u'' and len(token) >= 3:\n",
    "            new_text.append(token)\n",
    "    text_wo_stops = []\n",
    "    for token in new_text:\n",
    "        if not token in stopwords.words('english'):\n",
    "            text_wo_stops.append(token.lower())\n",
    "    return ' '.join(text_wo_stops)\n",
    "    \n",
    "clean_text(df_russian.loc[20, 'text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_russian = [clean_text(x) for x in train_russian_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_elect = [clean_text(x) for x in train_election_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_russian + X_elect\n",
    "with open(\"X_train.txt\", \"wb\") as fp:\n",
    "    pickle.dump(X_train, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_russian = [clean_text(x) for x in test_russian_df]\n",
    "X_elect = [clean_text(x) for x in test_election_df]\n",
    "X_test = X_russian + X_elect\n",
    "with open(\"X_test.txt\", \"wb\") as fp:\n",
    "    pickle.dump(X_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent a troll tweet with a 0, and a non troll tweet with a 1\n",
    "labels_train = ([0] * len(train_russian_df)) + ([1] * len(train_election_df))\n",
    "labels_test = ([0] * len(test_russian_df)) + ([1] * len(test_election_df))\n",
    "with open(\"labels_train.txt\", \"wb\") as fp:\n",
    "    pickle.dump(labels_train, fp)\n",
    "with open(\"labels_test.txt\", \"wb\") as fp:\n",
    "    pickle.dump(labels_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"X_train.txt\", \"rb\") as fp:\n",
    "    X_train = pickle.load(fp)\n",
    "with open(\"X_test.txt\", \"rb\") as fp:\n",
    "    X_test = pickle.load(fp)\n",
    "with open(\"labels_train.txt\", \"rb\") as fp:\n",
    "    labels_train = pickle.load(fp)\n",
    "with open(\"labels_test.txt\", \"rb\") as fp:\n",
    "    labels_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now, we move to the task of assigning indices to words, and filtering out infrequent words. For ease of use, we will include a parameter specifying the maximum number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train + X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these index encodings, we now obtain sequences from each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pad all of the tweets so that they have a length of 50 words. This will make it far easier to train embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 50\n",
    "train_data = pad_sequences(train_seq, maxlen=MAX_SEQ_LENGTH)\n",
    "test_data = pad_sequences(test_seq, maxlen=MAX_SEQ_LENGTH)\n",
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data: (480888, 50)\n",
      "shape of labels: (480888,)\n"
     ]
    }
   ],
   "source": [
    "print('shape of data:', train_data.shape)\n",
    "print('shape of labels:', labels_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "In order to extract semantic information from our tweets, we will use a word embedding matrix. However, word embeddings take an extremely long time to train. Therefore, we will use pretrained embeddings from GloVe, which are specifically trained on a set of 2 billion tweets (link [here](https://nlp.stanford.edu/projects/glove/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before gathering the embeddings, they need to be transferred into word2vec format, as they are currently in GloVe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18796148, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_FILE = '~/word2vec_twitter_model/word2vec_twitter_model.bin'\n",
    "RESULT_FILE = '~/word2vec_twitter_model/word2vec_twitter_model_formatted.txt'\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(EMBEDDING_FILE, RESULT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \n",
    "                                             binary=True,\n",
    "                                            unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "# tells us our embeddings are of size 400\n",
    "print(word2vec.word_vec('obama').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "num_words = min(len(word_index), max_words)\n",
    "EMBEDDING_DIM = 400\n",
    "\n",
    "t = 0\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, index in word_index.items():\n",
    "    t += 1\n",
    "    if word in word2vec.vocab and t < max_words:\n",
    "        embedding_matrix[index - 1] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.34817043  0.04459979  0.10225909 ... -0.61556971 -0.1482826\n",
      "   0.03635837]\n",
      " [-0.31657833  0.4608396   0.10638465 ...  0.05027082  0.02656249\n",
      "   0.15543474]\n",
      " [-0.21830973 -0.07237566 -0.24116141 ... -0.29306951 -0.23784301\n",
      "   0.0479187 ]\n",
      " ...\n",
      " [-0.26647705 -0.42137793  0.12377683 ... -0.1401113   0.31293917\n",
      "  -0.16521734]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formatting\n",
    "\n",
    "Next, we need to format our data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "perm = np.random.permutation(len(train_data))\n",
    "idx_train = perm[:int(len(train_data)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(train_data)*(1 - VALIDATION_SPLIT)):]\n",
    "data_train = train_data[idx_train]\n",
    "data_val = train_data[idx_val]\n",
    "train_labels = labels_train[idx_train]\n",
    "val_labels = labels_train[idx_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384710, 50)\n",
      "(384710,)\n",
      "(96178, 50)\n",
      "(96178,)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(train_labels.shape)\n",
    "print(data_val.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model\n",
    "\n",
    "Now that the data is properly formatted, it is time to build the actual model. We will be using Keras' sequential model, as it represents a linear stack of layers. While our model will not be exactly linear due to the recurrent structure of LSTMs, once LSTMs are unfolded, the structure actually will be \"linear\" in terms of the connections from one layer to the next. Then, the structure of our model is as follows. \n",
    "\n",
    "The first layer is an embedding layer which serves to find an embedding for a given tokenized tweet. This embedding is then fed into a 1-dimensional convolutional layer and 1D max pooling layer, which serves to reduce dimensionality (and therefore training time). We will also use a dropout layer here for regularization. After this, the network feeds into an LSTM layer, followed by a few fully connected layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 400)           8000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 48, 64)            76864     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 8,147,965\n",
      "Trainable params: 147,965\n",
      "Non-trainable params: 8,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(20000,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 384710 samples, validate on 96178 samples\n",
      "Epoch 1/10\n",
      "384710/384710 [==============================] - 246s 641us/step - loss: 0.1469 - acc: 0.9359 - val_loss: 0.1207 - val_acc: 0.9502\n",
      "Epoch 2/10\n",
      "384710/384710 [==============================] - 244s 634us/step - loss: 0.1176 - acc: 0.9506 - val_loss: 0.1068 - val_acc: 0.9553\n",
      "Epoch 3/10\n",
      "384710/384710 [==============================] - 244s 634us/step - loss: 0.1087 - acc: 0.9546 - val_loss: 0.1018 - val_acc: 0.9574\n",
      "Epoch 4/10\n",
      "384710/384710 [==============================] - 248s 643us/step - loss: 0.1024 - acc: 0.9575 - val_loss: 0.0998 - val_acc: 0.9585\n",
      "Epoch 5/10\n",
      "384710/384710 [==============================] - 244s 635us/step - loss: 0.0974 - acc: 0.9599 - val_loss: 0.0988 - val_acc: 0.9595\n",
      "Epoch 6/10\n",
      "384710/384710 [==============================] - 246s 639us/step - loss: 0.0935 - acc: 0.9616 - val_loss: 0.0954 - val_acc: 0.9605\n",
      "Epoch 7/10\n",
      "384710/384710 [==============================] - 246s 639us/step - loss: 0.0908 - acc: 0.9628 - val_loss: 0.0960 - val_acc: 0.9604\n",
      "Epoch 8/10\n",
      "384710/384710 [==============================] - 245s 636us/step - loss: 0.0881 - acc: 0.9643 - val_loss: 0.0935 - val_acc: 0.9621\n",
      "Epoch 9/10\n",
      "384710/384710 [==============================] - 246s 640us/step - loss: 0.0858 - acc: 0.9652 - val_loss: 0.0952 - val_acc: 0.9611\n",
      "Epoch 10/10\n",
      "384710/384710 [==============================] - 245s 636us/step - loss: 0.0829 - acc: 0.9668 - val_loss: 0.0959 - val_acc: 0.9616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00b981a908>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_train, train_labels, validation_data=(data_val, val_labels), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we save the model and its weights so that we will not have to retrain the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Results\n",
    "\n",
    "We trained the model on the dataset for 10 epochs. From the very first epoch, we found a very high accuracy, which was very good; however, since we continued to train the model for 10 epochs, and there was not much change in accuracy, there might be signs of overfitting. Below we show a variety of tests and analyses that test the results of our data. The first is a classification report, where we can see how our test data performed with the model. Following that, we showed a few of the actual tweets that we classified as \"Troll\" and a few we classified as \"Not Troll\" and compared it to the actual classification. Since we have a 95% accuracy, most of these were classified correctly. Below these, we see a few of the misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9545\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.94      0.93     40697\n",
      "          1       0.97      0.96      0.97     79526\n",
      "\n",
      "avg / total       0.95      0.95      0.95    120223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_model.predict(test_data)\n",
    "y_pred = [0 if x[0] < 0.5 else 1 for x in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(labels_test, y_pred)\n",
    "print('accuracy: ', round(accuracy, 4))\n",
    "print(classification_report(labels_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_yes = list(filter(lambda x: x[1] == 0, enumerate(y_pred)))\n",
    "y_pred_no = list(filter(lambda x: x[1] != 0, enumerate(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we classified as 'troll':\n",
      "Predicted: Troll -> Actual: Troll\n",
      "Tweet: @chriseddings5 @strhon2016 @dianeshamlin @charliekirk11 @right2liberty and let's forget bill clinton rapist ...\n",
      "\n",
      "Predicted: Troll -> Actual: Troll\n",
      "Tweet: @chadstanton defensively yell mama say i'm handsome https://t.co/k6jpuqdrlt\n",
      "\n",
      "Predicted: Troll -> Actual: Troll\n",
      "Tweet: i've told cheater always cheater got pride got\n",
      "\n",
      "Predicted: Troll -> Actual: Not Troll\n",
      "Tweet: unfortunately russia's players union left ... @fifpro prefers sponsor agents ... right https://t.co/vo4747cphw\n",
      "\n",
      "Predicted: Troll -> Actual: Troll\n",
      "Tweet: @realdonaldtrump i'm work hard never let make america great again https://t.co/cwfeceussq\n",
      "\n",
      "Predicted: Troll -> Actual: Troll\n",
      "Tweet: @ramcoban the truth behind oral anal http://t.co/l2imljzn8p\n",
      "\n",
      "Predicted: Troll -> Actual: Troll\n",
      "Tweet: funny cnn attack matthew mcconaughey immediately refused campaign trump hypocrit https://t.co/zydefzbgd1\n",
      "\n",
      "Predicted: Troll -> Actual: Troll\n",
      "Tweet: @nine_oh #nowplaying @asimsujud for only eyes tune https://t.co/ichlzw8eco\n",
      "\n",
      "Predicted: Troll -> Actual: Troll\n",
      "Tweet: @realdonaldtrump thank you another wonderful evening washington together make america great again https://t.co/â€¦\n",
      "\n",
      "Predicted: Troll -> Actual: Troll\n",
      "Tweet: hillary give damn refugees\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"What we classified as 'troll':\")\n",
    "for i, x in shuffle(y_pred_yes)[:10]:\n",
    "    print('Predicted: {:} -> Actual: {:}'.format('Not Troll' if x else 'Troll',\\\n",
    "                                                 'Not Troll' if labels_test[i] else 'Troll'))\n",
    "    print('Tweet: {:}\\n'.format(X_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we classified as 'Not Troll':\n",
      "Predicted: Not Troll -> Actual: Not Troll\n",
      "Tweet: republican democrat electioneers look voters south river high school @capgaznews #annearundelvotes https://t.co/6xopwgjzzo\n",
      "\n",
      "Predicted: Not Troll -> Actual: Not Troll\n",
      "Tweet: congress think detract modiji minority caste orrss hindutva othr weapons bcoz need https://t.co/n7gkhzussd\n",
      "\n",
      "Predicted: Not Troll -> Actual: Not Troll\n",
      "Tweet: grandma alive she'd excited vote hillary i'd hate hear vile things guy #election2016\n",
      "\n",
      "Predicted: Not Troll -> Actual: Not Troll\n",
      "Tweet: boi turning party https://t.co/dzmhgqoelu\n",
      "\n",
      "Predicted: Not Troll -> Actual: Not Troll\n",
      "Tweet: hope represents lot people check results https://t.co/oa7oto2hwe\n",
      "\n",
      "Predicted: Not Troll -> Actual: Not Troll\n",
      "Tweet: michigan sends republicans democrats congress https://t.co/nfcfho1zxq\n",
      "\n",
      "Predicted: Not Troll -> Actual: Not Troll\n",
      "Tweet: still relevant #election2016 https://t.co/map3g41edj\n",
      "\n",
      "Predicted: Not Troll -> Actual: Not Troll\n",
      "Tweet: #nation republicans keep control congress cnn https://t.co/fmjhpneuh6 https://t.co/cwjbaihmhy\n",
      "\n",
      "Predicted: Not Troll -> Actual: Not Troll\n",
      "Tweet: y'all vote democrat house senate talk wtf\n",
      "\n",
      "Predicted: Not Troll -> Actual: Not Troll\n",
      "Tweet: reluctant imaginary concession speech #election2016 #art #portrait #america #drawing #color https://t.co/tymszvrdbw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"What we classified as 'Not Troll':\")\n",
    "for i, x in shuffle(y_pred_no)[:10]:\n",
    "    print('Predicted: {:} -> Actual: {:}'.format('Not Troll' if x else 'Troll',\\\n",
    "                                                 'Not Troll' if labels_test[i] else 'Troll'))\n",
    "    print('Tweet: {:}\\n'.format(X_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we misclassified:\n",
      "Predicted: Troll -> Actual: Not Troll\n",
      "Tweet: pull florida https://t.co/jyozsz5p1w\n",
      "\n",
      "Predicted: Troll -> Actual: Not Troll\n",
      "Tweet: still amazed newspapers allowed think acceptable endorse candidates https://t.co/bwonyukhyz\n",
      "\n",
      "Predicted: Troll -> Actual: Not Troll\n",
      "Tweet: soros really angry https://t.co/yvgyuqn67k\n",
      "\n",
      "Predicted: Not Troll -> Actual: Troll\n",
      "Tweet: computer programmer testifies oath coded computers rig elections you voice important #riggedsystem https://t.co/x8wjwv8end\n",
      "\n",
      "Predicted: Not Troll -> Actual: Troll\n",
      "Tweet: @emenogu_phil https://t.co/kxo8kqu9vv\n",
      "\n",
      "Predicted: Troll -> Actual: Not Troll\n",
      "Tweet: Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±ÙŠÙˆÙ† ÙŠØ³ÙŠØ·Ø±ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆÙ†ØºØ±Ø³ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø¨Ø¹Ø¯ Ø¥Ø¹ØµØ§Ø± ØªØ±Ø§Ù…Ø¨ https://t.co/cfvwmjpcwj\n",
      "\n",
      "Predicted: Not Troll -> Actual: Troll\n",
      "Tweet: mass media president https://t.co/fp9o8h1xut\n",
      "\n",
      "Predicted: Not Troll -> Actual: Troll\n",
      "Tweet: cop-hater shot two cops four civilians philadelphia much #blacklivesmatter movement https://t.co/dfqravsfyf\n",
      "\n",
      "Predicted: Troll -> Actual: Not Troll\n",
      "Tweet: pÃ¡senle votar ... https://t.co/ev5b4qfils\n",
      "\n",
      "Predicted: Troll -> Actual: Not Troll\n",
      "Tweet: gracias por esa economia obama https://t.co/crwed8i49a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_mis = list(filter(lambda x: x[1] != labels_test[x[0]], enumerate(y_pred)))\n",
    "print(\"What we misclassified:\")\n",
    "for i, x in shuffle(y_pred_mis)[:10]:\n",
    "    print('Predicted: {:} -> Actual: {:}'.format('Not Troll' if x else 'Troll',\\\n",
    "                                                 'Not Troll' if labels_test[i] else 'Troll'))\n",
    "    print('Tweet: {:}\\n'.format(X_test[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
